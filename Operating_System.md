# Operating Systems 

## What is an Operating System?

*Definition*: An Operating System is system software that manages computer hardware, software resources, and provides common services for computer programs.

A *manager* that coordinates between applications and the computer's resources (CPU, memory, storage, devices).

*Key Functions*:
- *Resource Management*: CPU, memory, storage, I/O devices
- *Process Management*: Running and controlling programs
- *File System Management*: Organizing and accessing files
- *Security*: Protecting system and user data
- *User Interface*: Command line or graphical interface

### What happens if there is no OS?
- resource exploitation - 1 App will use all the resources 
- memory has no protection
- bulky and complex app - hardware interaction code must be in app's code base

## Core OS Components

### 1. Kernel
It is core of the OS that directly interacts with hardware

*Functions*:
- Process scheduling
- Memory management
- Device drivers
- System calls
- Interrupt handling

*Types of Kernels*:
- *Monolithic*: All services run in kernel space (Linux, Windows)
- *Microkernel*: Minimal kernel, services run in user space (QNX, Minix)
- *Hybrid*: Combination approach (macOS, Windows NT)

### 2. Shell
It is interface between user and kernel

*Types*:
- *Command Line Interface (CLI)*: bash, zsh, PowerShell
- *Graphical User Interface (GUI)*: Windows Explorer, macOS Finder

### 3. System Calls
It is interface between user programs and kernel services

*Examples*:

**File operations** : open(), read(), write(), close()

**Process operations** : fork(), exec(), wait(), exit()

**Memory operations**: malloc(), free(), mmap()


## Process Management

### What is a Process?
*Definition*: A program in execution, including:
- Program code (text section)
- Current activity (program counter, registers)
- Stack (temporary data)
- Heap (dynamically allocated memory)
- Data section (global variables)

### Process States

New → Ready → Running → Waiting → Terminated


1. *New*: Process being created
2. *Ready*: Waiting to be assigned to processor
3. *Running*: Instructions being executed
4. *Waiting*: Waiting for I/O or event
5. *Terminated*: Execution completed

### Process Control Block (PCB)
*Contains*:
- Process ID (PID)
- Program counter
- CPU registers
- Memory management info
- I/O status information
- Scheduling information

### Process Scheduling

#### CPU Scheduling Algorithms

*1. First Come First Serve (FCFS)*
- *Logic*: Execute processes in arrival order
- *Pros*: Simple, fair
- *Cons*: Poor average waiting time
- *Use case*: Batch systems

*2. Shortest Job First (SJF)*
- *Logic*: Execute shortest process first
- *Pros*: Optimal average waiting time
- *Cons*: May cause starvation, need to know execution time
- *Variants*: Preemptive (SRTF) and Non-preemptive

*3. Round Robin (RR)*
- *Logic*: Each process gets fixed time slice (quantum)
- *Pros*: Fair, good response time
- *Cons*: Higher overhead due to context switching
- *Use case*: Time-sharing systems

Time Quantum = 4ms
Processes: P1(10ms), P2(4ms), P3(2ms)

Execution: P1(4) → P2(4) → P3(2) → P1(4) → P1(2)


*4. Priority Scheduling*
- *Logic*: Execute highest priority process first
- *Problem*: Starvation of low-priority processes
- *Solution*: Aging (gradually increase priority)

*5. Multilevel Queue*
- *Logic*: Different queues for different process types
- *Example*: System processes, interactive processes, batch processes

### Context Switching
*What*: Saving state of current process and loading state of next process

*Steps*:
1. Save current process state in PCB
2. Update PCB (state = ready/waiting)
3. Move PCB to appropriate queue
4. Select new process to run
5. Load new process state from PCB
6. Update memory management structures

*Overhead*: Time spent in context switching is pure overhead

## Memory Management

### Memory Hierarchy

CPU Registers (fastest, smallest)→ Cache Memory (L1, L2, L3) →  
Main Memory (RAM)→ Secondary Storage (HDD/SSD)→ Tertiary Storage (slowest, largest)


### Memory Management Techniques

#### 1. Contiguous Memory Allocation

*Fixed Partitioning*
- Memory divided into fixed-size partitions
- *Problem*: Internal fragmentation

*Dynamic Partitioning*
- Memory allocated exactly as needed
- *Problem*: External fragmentation

*Allocation Algorithms*:
- *First Fit*: Allocate first available block
- *Best Fit*: Allocate smallest sufficient block
- *Worst Fit*: Allocate largest available block

#### 2. Paging
*Concept*: Divide memory into fixed-size blocks (pages)

*Components*:
- *Logical Address*: Generated by CPU
- *Physical Address*: Actual memory location
- *Page Table*: Maps logical to physical addresses


Logical Address = Page Number + Page Offset
Physical Address = Frame Number + Page Offset


*Benefits*:
- No external fragmentation
- Allows non-contiguous allocation
- Enables virtual memory

*Translation Lookaside Buffer (TLB)*:
- Cache for page table entries
- Speeds up address translation

#### 3. Segmentation
*Concept*: Divide memory based on logical units (code, data, stack)

*Benefits*:
- Matches programmer's view
- Easy sharing and protection
- Dynamic sizing

*Drawbacks*:
- External fragmentation
- Complex memory management

#### 4. Virtual Memory
*Concept*: Allow programs larger than physical memory to run

*Implementation*: Demand Paging
- Load pages only when needed
- Use secondary storage as extension of main memory

*Page Replacement Algorithms*:

*FIFO (First In First Out)*
- Replace oldest page
- Simple but not optimal

*LRU (Least Recently Used)*
- Replace page not used for longest time
- Good performance, expensive to implement

*Optimal*
- Replace page that won't be used for longest time
- Theoretical best, impossible to implement

*Clock Algorithm*
- Approximation of LRU using reference bits
- Circular list with pointer

### Thrashing
*Problem*: System spends more time swapping pages than executing
*Cause*: Too many processes in memory
*Solution*: Reduce multiprogramming degree

## File Systems

### File Operations
- *Create*: Make new file
- *Open*: Prepare file for access
- *Read*: Get data from file
- *Write*: Put data to file
- *Seek*: Change current position
- *Close*: Finish file access
- *Delete*: Remove file

### Directory Structure
*Single-level*: All files in one directory
*Two-level*: User directories under root
*Tree*: Hierarchical structure (most common)
*Graph*: Allows links and shortcuts

### File Allocation Methods

#### 1. Contiguous Allocation
- *Method*: Store files in consecutive blocks
- *Pros*: Fast access, simple
- *Cons*: External fragmentation, difficult to grow files

#### 2. Linked Allocation
- *Method*: Each block points to next block
- *Pros*: No fragmentation, easy to grow
- *Cons*: Sequential access only, overhead of pointers

#### 3. Indexed Allocation
- *Method*: Index block contains pointers to data blocks
- *Pros*: Direct access, no fragmentation
- *Cons*: Overhead of index blocks

### Modern File Systems
- *NTFS* (Windows): Journaling, compression, encryption
- *ext4* (Linux): Journaling, large file support
- *APFS* (macOS): Copy-on-write, snapshots
- *ZFS*: Advanced features, data integrity

## Synchronization

### The Critical Section Problem
*Problem*: Multiple processes accessing shared resources simultaneously

*Requirements*:
1. *Mutual Exclusion*: Only one process in critical section
2. *Progress*: Selection of next process cannot be postponed indefinitely
3. *Bounded Waiting*: Limited waiting time for any process

### Synchronization Tools

#### 1. Mutex (Mutual Exclusion)
```
mutex_lock(&lock);
// Critical Section
mutex_unlock(&lock);
```

#### 2. Semaphore
*Binary Semaphore*: 0 or 1 (like mutex)
*Counting Semaphore*: Can have value > 1

```
// Producer-Consumer with Semaphore
semaphore empty = n;    // Empty slots
semaphore full = 0;     // Full slots  
semaphore mutex = 1;    // Critical section

Producer:
    wait(empty);
    wait(mutex);
    // Add item to buffer
    signal(mutex);
    signal(full);

Consumer:
    wait(full);
    wait(mutex);
    // Remove item from buffer
    signal(mutex);
    signal(empty);
```

#### 3. Monitor
*Concept*: High-level synchronization construct
*Features*: Mutual exclusion built-in, condition variables

### Classic Synchronization Problems

#### 1. Producer-Consumer Problem
- *Scenario*: Producer creates data, consumer uses data
- *Issues*: Buffer overflow, buffer underflow
- *Solution*: Semaphores or monitors

#### 2. Readers-Writers Problem
- *Scenario*: Multiple readers, exclusive writers
- *Constraint*: Multiple readers OR one writer
- *Solutions*: Reader priority, writer priority, fair

#### 3. Dining Philosophers Problem
- *Scenario*: 5 philosophers, 5 chopsticks, need 2 to eat
- *Issue*: Deadlock if all pick up left chopstick
- *Solutions*: Asymmetric solution, resource ordering

### Deadlock

#### Necessary Conditions (Coffman Conditions)
1. *Mutual Exclusion*: Resources cannot be shared
2. *Hold and Wait*: Process holds resources while waiting
3. *No Preemption*: Resources cannot be forcibly taken
4. *Circular Wait*: Circular chain of waiting processes

#### Deadlock Prevention
- *Eliminate Mutual Exclusion*: Make resources sharable
- *Eliminate Hold and Wait*: Request all resources at once
- *Allow Preemption*: Take resources from processes
- *Eliminate Circular Wait*: Order resources, request in order

#### Deadlock Avoidance
*Banker's Algorithm*: Check if granting request leads to safe state

#### Deadlock Detection and Recovery
- *Detection*: Build resource allocation graph
- *Recovery*: Kill processes or preempt resources

## I/O Systems

### I/O Hardware
- *I/O Ports*: Communication endpoints
- *Buses*: Data pathways (PCI, USB, SATA)
- *Controllers*: Hardware that manages devices

### I/O Methods

#### 1. Programmed I/O (Polling)
- CPU constantly checks device status
- *Pros*: Simple
- *Cons*: CPU waste, poor performance

#### 2. Interrupt-Driven I/O
- Device sends interrupt when ready
- *Pros*: CPU can do other work
- *Cons*: High overhead for bulk transfers

#### 3. Direct Memory Access (DMA)
- Device transfers data directly to/from memory
- *Pros*: Minimal CPU involvement
- *Cons*: More complex hardware

### I/O Scheduling
*Purpose*: Optimize disk access patterns

*Algorithms*:
- *FCFS*: First come, first served
- *SCAN*: Elevator algorithm - sweep across disk
- *C-SCAN*: Circular SCAN - always sweep in one direction
- *LOOK*: SCAN but only go as far as needed

## Security and Protection

### Security Goals
- *Confidentiality*: Information not disclosed to unauthorized users
- *Integrity*: Information not modified by unauthorized users  
- *Availability*: Information accessible when needed

### Protection Mechanisms
- *Access Control Lists (ACL)*: List of permissions per object
- *Capability Lists*: List of permissions per user
- *Role-Based Access Control (RBAC)*: Permissions based on roles

### Authentication Methods
- *Something you know*: Passwords, PINs
- *Something you have*: Smart cards, tokens
- *Something you are*: Biometrics (fingerprint, iris)


##

### Q: What's the difference between process and thread?
*Answer*: 
- *Process*: Independent program with own memory space, expensive to create/switch
- *Thread*: Lightweight execution unit within process, shares memory, cheaper to create/switch
- *Example*: Browser (process) with multiple tabs (threads)

### Q: Explain virtual memory
*Answer*: Virtual memory allows programs larger than physical RAM to run by using disk storage as extension. When memory is needed, pages are loaded from disk. When memory is full, least recently used pages are swapped out. This provides illusion of unlimited memory.

### Q: What causes thrashing and how to prevent it?
*Answer*: Thrashing occurs when system spends more time swapping pages than executing. Caused by too many processes competing for memory. Prevention: reduce multiprogramming degree, increase memory, improve page replacement algorithm, or use working set model.

### Q: Difference between preemptive and non-preemptive scheduling?
*Answer*: 
- *Preemptive*: OS can interrupt running process (Round Robin, Priority with preemption)
- *Non-preemptive*: Process runs until completion or voluntary yield (FCFS, SJF)
- *Trade-off*: Preemptive gives better response time but higher overhead

### Q: How does paging work?
*Answer*: Paging divides memory into fixed-size blocks (pages in logical memory, frames in physical memory). 

Page table maps logical pages to physical frames. CPU generates logical address (page number + offset), MMU translates to physical address using page table. 

TLB caches recent translations for speed.

### Q: Explain the producer-consumer problem
*Answer*: Producer creates data and puts in buffer, consumer takes data from buffer. Problems: buffer overflow (producer faster), buffer underflow (consumer faster). Solution using semaphores: empty semaphore tracks free slots, full semaphore tracks occupied slots, mutex protects critical section.
